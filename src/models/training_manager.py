# è¨“ç·´ç®¡ç†å™¨
"""
è¨“ç·´ç®¡ç†å™¨æ¨¡çµ„

æä¾›å®Œæ•´çš„æ¨¡å‹è¨“ç·´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æå¤±å‡½æ•¸å®šç¾©
- å„ªåŒ–å™¨é…ç½®
- å­¸ç¿’ç‡èª¿åº¦å™¨
- æ—©åœæ©Ÿåˆ¶
- è¨“ç·´éç¨‹ç®¡ç†
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau
from typing import Dict, List, Optional, Any, Callable
import numpy as np
from pathlib import Path
import json


class TrainingManager:
    """è¨“ç·´ç®¡ç†å™¨"""
    
    def __init__(self,
                 model: nn.Module,
                 train_loader,
                 val_loader,
                 config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è¨“ç·´ç®¡ç†å™¨
        
        Args:
            model: è¦è¨“ç·´çš„æ¨¡å‹
            train_loader: è¨“ç·´æ•¸æ“šè¼‰å…¥å™¨
            val_loader: é©—è­‰æ•¸æ“šè¼‰å…¥å™¨
            config: è¨“ç·´é…ç½®
        """
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # è¨­å‚™é…ç½®
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        self.model.to(self.device)
        
        # å„ªåŒ–å™¨è¨­ç½®
        self.optimizer = self._setup_optimizer()
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = self._setup_scheduler()
        
        # æå¤±å‡½æ•¸
        self.criterion = self._setup_criterion()
        
        # æ—©åœè¨­ç½®
        self.early_stopping = EarlyStopping(
            patience=config.get('patience', 10),
            min_delta=config.get('min_delta', 1e-4),
            restore_best_weights=config.get('restore_best_weights', True)
        )
        
        # è¨“ç·´æ­·å²
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def _setup_optimizer(self) -> optim.Optimizer:
        """è¨­ç½®å„ªåŒ–å™¨"""
        optimizer_name = self.config.get('optimizer', 'adam').lower()
        learning_rate = self.config.get('learning_rate', 2e-5)
        weight_decay = self.config.get('weight_decay', 1e-4)
        
        if optimizer_name == 'adam':
            return optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif optimizer_name == 'adamw':
            return optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif optimizer_name == 'sgd':
            momentum = self.config.get('momentum', 0.9)
            return optim.SGD(self.model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
        else:
            return optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    def _setup_scheduler(self):
        """è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨"""
        scheduler_name = self.config.get('scheduler', 'reduce_on_plateau').lower()
        
        if scheduler_name == 'step':
            step_size = self.config.get('step_size', 10)
            gamma = self.config.get('gamma', 0.1)
            return StepLR(self.optimizer, step_size=step_size, gamma=gamma)
        
        elif scheduler_name == 'cosine':
            T_max = self.config.get('T_max', 50)
            return CosineAnnealingLR(self.optimizer, T_max=T_max)
        
        elif scheduler_name == 'reduce_on_plateau':
            factor = self.config.get('factor', 0.1)
            patience = self.config.get('scheduler_patience', 5)
            return ReduceLROnPlateau(self.optimizer, mode='min', factor=factor, patience=patience)
        
        else:
            return ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=5)
    
    def _setup_criterion(self):
        """è¨­ç½®æå¤±å‡½æ•¸"""
        criterion_name = self.config.get('criterion', 'cross_entropy').lower()
        
        if criterion_name == 'cross_entropy':
            return nn.CrossEntropyLoss()
        elif criterion_name == 'focal':
            return FocalLoss(alpha=1, gamma=2)
        elif criterion_name == 'label_smoothing':
            smoothing = self.config.get('label_smoothing', 0.1)
            return LabelSmoothingCrossEntropy(smoothing=smoothing)
        else:
            return nn.CrossEntropyLoss()
    
    def _check_and_fix_model_dimensions(self, inputs):
        """æª¢æŸ¥ä¸¦ä¿®å¾©æ¨¡å‹ç¶­åº¦ä¸åŒ¹é…å•é¡Œ"""
        try:
            # è¨ˆç®—å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼ˆç§»é™¤è©³ç´°é™¤éŒ¯è¼¸å‡ºï¼‰
            if isinstance(inputs, dict):
                feature_tensors = []
                for key in sorted(inputs.keys()):
                    feature = inputs[key]
                    
                    # ç¢ºä¿ç‰¹å¾µæ˜¯å¼µé‡
                    if isinstance(feature, torch.Tensor):
                        # å¦‚æœæ˜¯å¤šç¶­å¼µé‡ï¼Œå±•å¹³é™¤äº†æ‰¹æ¬¡ç¶­åº¦
                        if feature.dim() > 2:
                            feature = feature.view(feature.size(0), -1)
                        feature_tensors.append(feature)
                    else:
                        continue
                
                if not feature_tensors:
                    return
                    
                # è¨ˆç®—æ‹¼æ¥å¾Œçš„ç¸½ç¶­åº¦
                individual_dims = [t.size(-1) for t in feature_tensors]
                actual_dim = sum(individual_dims)
            else:
                actual_dim = inputs.size(-1)
            
            # æª¢æŸ¥æ¨¡å‹é¡å‹å’Œé‡æ–°åˆå§‹åŒ–æ–¹æ³•
            model_reinitialized = False
            
            # æª¢æŸ¥ä¸¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆåƒ…åœ¨éœ€è¦æ™‚é¡¯ç¤ºè¨Šæ¯ï¼‰
            if hasattr(self.model, 'reinitialize_for_input_dim'):
                model_reinitialized = self.model.reinitialize_for_input_dim(actual_dim)
                # å¼·åˆ¶æ›´æ–°é æœŸç¶­åº¦
                if hasattr(self.model, '_expected_input_dim'):
                    self.model._expected_input_dim = actual_dim
                
            elif hasattr(self.model, '__len__') and len(self.model) >= 1:
                # è™•ç†Sequentialæ¨¡å‹ï¼Œç¬¬ä¸€å±¤æ˜¯MultiModalFeatureFusion
                first_layer = self.model[0]
                if hasattr(first_layer, 'reinitialize_for_input_dims'):
                    actual_feature_dims = first_layer.calculate_actual_feature_dims(inputs)
                    fusion_reinitialized = first_layer.reinitialize_for_input_dims(actual_feature_dims)
                    
                    # å¦‚æœèåˆå±¤é‡æ–°åˆå§‹åŒ–ï¼ŒåŒæ™‚é‡æ–°åˆå§‹åŒ–åˆ†é¡å™¨
                    if fusion_reinitialized and len(self.model) >= 2:
                        classifier = self.model[1]
                        if hasattr(classifier, 'reinitialize_for_input_dim'):
                            fusion_output_dim = first_layer.fusion_dim
                            classifier.reinitialize_for_input_dim(fusion_output_dim)
                    
                    model_reinitialized = fusion_reinitialized
                else:
                    model_reinitialized = False
            else:
                model_reinitialized = False
            
            if model_reinitialized:
                # é‡æ–°åˆå§‹åŒ–å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨
                print("ğŸ”„ æ¨¡å‹ç¶­åº¦å·²è‡ªå‹•èª¿æ•´ï¼Œé‡æ–°å»ºç«‹å„ªåŒ–å™¨...")
                self.optimizer = self._setup_optimizer()
                self.scheduler = self._setup_scheduler()
                print("âœ… å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨é‡æ–°å»ºç«‹å®Œæˆ")
            else:
                # é™¤éŒ¯ï¼šé¡¯ç¤ºç‚ºä»€éº¼æ²’æœ‰é‡æ–°åˆå§‹åŒ–
                expected_dim = getattr(self.model, '_expected_input_dim', 'æœªçŸ¥')
                print(f"ğŸ” é™¤éŒ¯è³‡è¨Šï¼šæ¨¡å‹é¡å‹ {type(self.model).__name__}, é æœŸç¶­åº¦: {expected_dim}, å¯¦éš›ç¶­åº¦: {actual_dim}")
                    
        except Exception as e:
            print(f"âŒ ç¶­åº¦æª¢æŸ¥å‡ºç¾éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

    def train_epoch(self) -> Dict[str, float]:
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        total_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        first_batch = True
        
        for batch in self.train_loader:
            # ç§»å‹•æ•¸æ“šåˆ°è¨­å‚™
            if isinstance(batch['features'], dict):
                # è™•ç†ç‰¹å¾µå­—å…¸ï¼šå°‡æ¯å€‹ç‰¹å¾µå¼µé‡ç§»å‹•åˆ°è¨­å‚™
                inputs = {key: tensor.to(self.device) for key, tensor in batch['features'].items()}
            else:
                # è™•ç†å–®ä¸€å¼µé‡
                inputs = batch['features'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # åœ¨ç¬¬ä¸€å€‹æ‰¹æ¬¡æª¢æŸ¥ä¸¦ä¿®å¾©ç¶­åº¦ä¸åŒ¹é…
            if first_batch:
                self._check_and_fix_model_dimensions(inputs)
                first_batch = False
            
            # å‰å‘å‚³æ’­
            self.optimizer.zero_grad()
            try:
                outputs = self.model(inputs)
            except RuntimeError as e:
                if "Input dimension mismatch" in str(e) or "è¼¸å…¥ç¶­åº¦ä¸åŒ¹é…" in str(e):
                    print(f"âŒ æ•ç²åˆ°ç¶­åº¦éŒ¯èª¤ï¼Œå˜—è©¦è‡ªå‹•ä¿®å¾©ï¼š{e}")
                    # å¦‚æœç¶­åº¦æª¢æŸ¥å¤±æ•—ï¼Œå†æ¬¡å˜—è©¦ä¿®å¾©
                    self._check_and_fix_model_dimensions(inputs)
                    print("ğŸ”„ å·²å˜—è©¦ä¿®å¾©ç¶­åº¦å•é¡Œï¼Œé‡æ–°åŸ·è¡Œå‰å‘å‚³æ’­...")
                    # ç¢ºä¿æ¨¡å‹åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
                    self.model = self.model.to(self.device)
                    try:
                        outputs = self.model(inputs)
                    except RuntimeError as retry_e:
                        print(f"âŒ é‡è©¦å¾Œä»ç„¶å¤±æ•—ï¼š{retry_e}")
                        print(f"ğŸ” æ¨¡å‹é æœŸç¶­åº¦ï¼š{getattr(self.model, '_expected_input_dim', 'æœªçŸ¥')}")
                        print(f"ğŸ” å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼š{inputs.size(-1) if not isinstance(inputs, dict) else 'å­—å…¸è¼¸å…¥'}")
                        raise retry_e
                else:
                    raise e
            
            # è¨ˆç®—æå¤±
            if isinstance(outputs, dict):
                logits = outputs['logits']
            else:
                logits = outputs
            
            loss = self.criterion(logits, labels)
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.get('gradient_clipping'):
                nn.utils.clip_grad_norm_(self.model.parameters(), self.config['gradient_clipping'])
            
            self.optimizer.step()
            
            # çµ±è¨ˆ
            total_loss += loss.item()
            predictions = torch.argmax(logits, dim=-1)
            correct_predictions += (predictions == labels).sum().item()
            total_samples += labels.size(0)
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = correct_predictions / total_samples
        
        return {'loss': avg_loss, 'accuracy': accuracy}
    
    def validate_epoch(self) -> Dict[str, float]:
        """é©—è­‰ä¸€å€‹epoch"""
        self.model.eval()
        total_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # ç§»å‹•æ•¸æ“šåˆ°è¨­å‚™
                if isinstance(batch['features'], dict):
                    # è™•ç†ç‰¹å¾µå­—å…¸ï¼šå°‡æ¯å€‹ç‰¹å¾µå¼µé‡ç§»å‹•åˆ°è¨­å‚™
                    inputs = {key: tensor.to(self.device) for key, tensor in batch['features'].items()}
                else:
                    # è™•ç†å–®ä¸€å¼µé‡
                    inputs = batch['features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(inputs)
                
                if isinstance(outputs, dict):
                    logits = outputs['logits']
                else:
                    logits = outputs
                
                loss = self.criterion(logits, labels)
                
                total_loss += loss.item()
                predictions = torch.argmax(logits, dim=-1)
                correct_predictions += (predictions == labels).sum().item()
                total_samples += labels.size(0)
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = correct_predictions / total_samples
        
        return {'loss': avg_loss, 'accuracy': accuracy}
    
    def train(self, epochs: int) -> Dict[str, List]:
        """åŸ·è¡Œå®Œæ•´è¨“ç·´"""
        print(f"é–‹å§‹è¨“ç·´ï¼Œå…± {epochs} å€‹ epoch")
        
        for epoch in range(epochs):
            print(f"\\nEpoch {epoch + 1}/{epochs}")
            print("-" * 50)
            
            # è¨“ç·´éšæ®µ
            train_metrics = self.train_epoch()
            print(f"è¨“ç·´ - æå¤±: {train_metrics['loss']:.4f}, æº–ç¢ºç‡: {train_metrics['accuracy']:.4f}")
            
            # é©—è­‰éšæ®µ
            val_metrics = self.validate_epoch()
            print(f"é©—è­‰ - æå¤±: {val_metrics['loss']:.4f}, æº–ç¢ºç‡: {val_metrics['accuracy']:.4f}")
            
            # æ›´æ–°æ­·å²
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['train_acc'].append(train_metrics['accuracy'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            
            # å­¸ç¿’ç‡èª¿åº¦
            if isinstance(self.scheduler, ReduceLROnPlateau):
                self.scheduler.step(val_metrics['loss'])
            else:
                self.scheduler.step()
            
            # æ—©åœæª¢æŸ¥
            if self.early_stopping(val_metrics['loss'], self.model):
                print(f"\\næ—©åœè§¸ç™¼ï¼Œåœ¨ç¬¬ {epoch + 1} epoch åœæ­¢è¨“ç·´")
                break
        
        return self.history
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'history': self.history,
            'config': self.config
        }
        torch.save(checkpoint, filepath)
        print(f"æª¢æŸ¥é»å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """è¼‰å…¥æª¢æŸ¥é»"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.history = checkpoint['history']
        print(f"æª¢æŸ¥é»å·²å¾ {filepath} è¼‰å…¥")


class EarlyStopping:
    """æ—©åœé¡"""
    
    def __init__(self, patience: int = 10, min_delta: float = 1e-4, restore_best_weights: bool = True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
    
    def __call__(self, val_loss: float, model: nn.Module) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ—©åœ"""
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
        
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights is not None:
                model.load_state_dict(self.best_weights)
                print("å·²æ¢å¾©æœ€ä½³æ¬Šé‡")
            return True
        
        return False


class FocalLoss(nn.Module):
    """Focal Loss å¯¦ç¾"""
    
    def __init__(self, alpha: float = 1, gamma: float = 2, reduction: str = 'mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingCrossEntropy(nn.Module):
    """æ¨™ç±¤å¹³æ»‘äº¤å‰ç†µæå¤±"""
    
    def __init__(self, smoothing: float = 0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing
    
    def forward(self, inputs, targets):
        log_prob = nn.functional.log_softmax(inputs, dim=-1)
        weight = inputs.new_ones(inputs.size()) * self.smoothing / (inputs.size(-1) - 1.)
        weight.scatter_(-1, targets.unsqueeze(-1), (1. - self.smoothing))
        loss = (-weight * log_prob).sum(dim=-1).mean()
        return loss