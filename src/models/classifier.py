# æƒ…æ„Ÿåˆ†é¡å™¨
"""
æƒ…æ„Ÿåˆ†é¡å™¨æ¨¡çµ„

æä¾›å¤šç¨®æƒ…æ„Ÿåˆ†é¡æ¶æ§‹ï¼ŒåŒ…æ‹¬ï¼š
- åŸºç¤åˆ†é¡å™¨
- å¤šå±¤æ„ŸçŸ¥æ©Ÿåˆ†é¡å™¨
- æ³¨æ„åŠ›å¢å¼·åˆ†é¡å™¨
- éšå±¤å¼åˆ†é¡å™¨
- è·¨é ˜åŸŸåˆ†é¡å™¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any, Union
import numpy as np
from abc import ABC, abstractmethod


class BaseClassifier(nn.Module, ABC):
    """
    åˆ†é¡å™¨åŸºç¤é¡
    
    æä¾›æ‰€æœ‰åˆ†é¡å™¨çš„é€šç”¨æ¥å£å’ŒåŠŸèƒ½
    """
    
    def __init__(self, 
                 input_dim: int,
                 num_classes: int,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–åŸºç¤åˆ†é¡å™¨
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            num_classes: åˆ†é¡æ•¸é‡
            dropout_rate: Dropoutæ¯”ç‡
        """
        super(BaseClassifier, self).__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        
        # é€šç”¨Dropoutå±¤
        self.dropout = nn.Dropout(dropout_rate)
        
    def _process_input_features(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> torch.Tensor:
        """
        è™•ç†è¼¸å…¥ç‰¹å¾µï¼Œå°‡å­—å…¸æ ¼å¼è½‰æ›ç‚ºå¼µé‡
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µæˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            è™•ç†å¾Œçš„å¼µé‡
        """
        if isinstance(x, dict):
            # å°‡æ‰€æœ‰ç‰¹å¾µæ‹¼æ¥æˆå–®ä¸€å¼µé‡
            feature_tensors = []
            for key in sorted(x.keys()):  # ä¿æŒé †åºä¸€è‡´æ€§
                feature = x[key]
                # ç¢ºä¿ç‰¹å¾µæ˜¯å¼µé‡
                if isinstance(feature, torch.Tensor):
                    # å¦‚æœæ˜¯å¤šç¶­å¼µé‡ï¼Œå±•å¹³é™¤äº†æ‰¹æ¬¡ç¶­åº¦
                    if feature.dim() > 2:
                        feature = feature.view(feature.size(0), -1)
                    feature_tensors.append(feature)
                elif isinstance(feature, np.ndarray):
                    feature_tensor = torch.from_numpy(feature).float()
                    if feature_tensor.dim() > 2:
                        feature_tensor = feature_tensor.view(feature_tensor.size(0), -1)
                    feature_tensors.append(feature_tensor)
                elif isinstance(feature, dict):
                    # å¦‚æœç‰¹å¾µæœ¬èº«æ˜¯å­—å…¸ï¼Œéæ­¸å±•å¹³è™•ç†
                    try:
                        flattened_features = self._flatten_dict_features(feature)
                        if flattened_features is not None and len(flattened_features) > 0:
                            feature_tensors.extend(flattened_features)
                        else:
                            print(f"Warning: å·¢ç‹€å­—å…¸ç‰¹å¾µ '{key}' ç„¡æœ‰æ•ˆå¼µé‡ï¼Œè·³é")
                    except Exception as e:
                        print(f"Warning: è™•ç†å·¢ç‹€å­—å…¸ç‰¹å¾µ '{key}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œè·³é")
                    continue
                else:
                    print(f"Warning: Skipping unsupported feature type '{key}': {type(feature)}")
                    continue
            
            if not feature_tensors:
                # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„ç‰¹å¾µå¼µé‡ï¼Œå‰µå»ºä¸€å€‹é›¶å¼µé‡
                batch_size = 1  # é»˜èªæ‰¹æ¬¡å¤§å°
                return torch.zeros(batch_size, 1)
            else:
                return torch.cat(feature_tensors, dim=-1)
        else:
            return x
    
    def _flatten_dict_features(self, feature_dict: Dict) -> List[torch.Tensor]:
        """
        éæ­¸å±•å¹³å­—å…¸ç‰¹å¾µ
        
        Args:
            feature_dict: åŒ…å«ç‰¹å¾µçš„å­—å…¸
            
        Returns:
            å±•å¹³å¾Œçš„å¼µé‡åˆ—è¡¨
        """
        feature_tensors = []
        
        for key, value in feature_dict.items():
            try:
                if isinstance(value, torch.Tensor):
                    # å¦‚æœæ˜¯å¤šç¶­å¼µé‡ï¼Œå±•å¹³é™¤äº†æ‰¹æ¬¡ç¶­åº¦
                    if value.dim() > 2:
                        value = value.view(value.size(0), -1)
                    feature_tensors.append(value)
                elif isinstance(value, np.ndarray):
                    tensor_value = torch.from_numpy(value).float()
                    if tensor_value.dim() > 2:
                        tensor_value = tensor_value.view(tensor_value.size(0), -1)
                    feature_tensors.append(tensor_value)
                elif isinstance(value, dict):
                    # éæ­¸è™•ç†å·¢ç‹€å­—å…¸
                    nested_tensors = self._flatten_dict_features(value)
                    feature_tensors.extend(nested_tensors)
                else:
                    # è·³éä¸æ”¯æ´çš„è³‡æ–™é¡å‹ï¼ˆä½†ä¸è¼¸å‡ºè­¦å‘Šï¼Œé¿å…éå¤šè¼¸å‡ºï¼‰
                    continue
            except Exception as e:
                print(f"Warning: è™•ç†å­—å…¸ç‰¹å¾µ '{key}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        return feature_tensors
        
    @abstractmethod
    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­æŠ½è±¡æ–¹æ³•
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µæˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            åˆ†é¡çµæœå­—å…¸
        """
        pass
        
    def predict(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> torch.Tensor:
        """
        é æ¸¬æ–¹æ³•
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µæˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            é æ¸¬æ¨™ç±¤
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(x)
            predictions = torch.argmax(outputs['logits'], dim=-1)
        return predictions
    
    def predict_proba(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> torch.Tensor:
        """
        é æ¸¬æ©Ÿç‡æ–¹æ³•
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µæˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            é æ¸¬æ©Ÿç‡
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(x)
            probabilities = F.softmax(outputs['logits'], dim=-1)
        return probabilities


class MLPClassifier(BaseClassifier):
    """
    å¤šå±¤æ„ŸçŸ¥æ©Ÿåˆ†é¡å™¨
    
    ä½¿ç”¨å¤šå±¤å…¨é€£æ¥ç¶²è·¯é€²è¡Œæƒ…æ„Ÿåˆ†é¡
    """
    
    def __init__(self,
                 input_dim: int,
                 num_classes: int,
                 hidden_dims: List[int] = [512, 256],
                 dropout_rate: float = 0.1,
                 activation: str = 'relu'):
        """
        åˆå§‹åŒ–MLPåˆ†é¡å™¨
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            num_classes: åˆ†é¡æ•¸é‡
            hidden_dims: éš±è—å±¤ç¶­åº¦åˆ—è¡¨
            dropout_rate: Dropoutæ¯”ç‡
            activation: æ¿€æ´»å‡½æ•¸é¡å‹
        """
        super(MLPClassifier, self).__init__(input_dim, num_classes, dropout_rate)
        
        self.hidden_dims = hidden_dims
        self.activation = activation
        self._expected_input_dim = input_dim
        
        # ç›´æ¥å»ºç«‹ç¶²è·¯å±¤
        self._build_network(input_dim)
        
    def _build_network(self, input_dim: int):
        """å»ºç«‹ç¶²è·¯å±¤"""
        # å»ºç«‹MLPå±¤
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in self.hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                self._get_activation(self.activation),
                nn.Dropout(self.dropout_rate)
            ])
            prev_dim = hidden_dim
        
        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_dim, self.num_classes))
        
        self.classifier = nn.Sequential(*layers)
        
        # ç‰¹å¾µæå–å±¤ï¼ˆè¼¸å‡ºå±¤ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
        self.feature_extractor = nn.Sequential(*layers[:-1])
        self.output_layer = layers[-1]
        
    def reinitialize_for_input_dim(self, actual_input_dim: int):
        """
        æ ¹æ“šå¯¦éš›è¼¸å…¥ç¶­åº¦é‡æ–°åˆå§‹åŒ–ç¶²è·¯ï¼ˆç•¶ç¶­åº¦ä¸åŒ¹é…æ™‚ä½¿ç”¨ï¼‰
        
        Args:
            actual_input_dim: å¯¦éš›è¼¸å…¥ç‰¹å¾µç¶­åº¦
        """
        if self._expected_input_dim != actual_input_dim:
            print(f"âš ï¸  ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸè¼¸å…¥ç¶­åº¦ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}")
            print("ğŸ”„ æ­£åœ¨ä½¿ç”¨æ­£ç¢ºç¶­åº¦é‡æ–°åˆå§‹åŒ–åˆ†é¡å™¨...")
            
            # ä¿å­˜ç•¶å‰è¨­å‚™
            device = next(self.parameters()).device
            
            # é‡å»ºç¶²è·¯
            self._expected_input_dim = actual_input_dim
            self._build_network(actual_input_dim)
            
            # ç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
            self.to(device)
            
            return True
        return False
    
    def _get_activation(self, activation: str) -> nn.Module:
        """
        ç²å–æ¿€æ´»å‡½æ•¸
        
        Args:
            activation: æ¿€æ´»å‡½æ•¸åç¨±
            
        Returns:
            æ¿€æ´»å‡½æ•¸æ¨¡çµ„
        """
        activations = {
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'leaky_relu': nn.LeakyReLU(),
            'elu': nn.ELU()
        }
        return activations.get(activation, nn.ReLU())
    
    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ [batch_size, input_dim] æˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            åˆ†é¡çµæœå­—å…¸
        """
        # è™•ç†è¼¸å…¥ç‰¹å¾µ
        x = self._process_input_features(x)
        
        # æª¢æŸ¥è¼¸å…¥ç¶­åº¦æ˜¯å¦èˆ‡æœŸæœ›çš„åŒ¹é…
        actual_input_dim = x.size(-1)
        if actual_input_dim != self._expected_input_dim:
            print(f"âš ï¸ å‰å‘å‚³æ’­ä¸­æª¢æ¸¬åˆ°ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}")
            print("ğŸ”„ è‡ªå‹•é‡æ–°åˆå§‹åŒ–æ¨¡å‹ç¶­åº¦...")
            # è‡ªå‹•é‡æ–°åˆå§‹åŒ–
            if hasattr(self, 'reinitialize_for_input_dim'):
                self.reinitialize_for_input_dim(actual_input_dim)
                print("âœ… æ¨¡å‹ç¶­åº¦é‡æ–°åˆå§‹åŒ–å®Œæˆ")
            else:
                raise RuntimeError(
                    f"è¼¸å…¥ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}ã€‚"
                    f"è«‹åœ¨è¨“ç·´å‰å‘¼å« reinitialize_for_input_dim() æ–¹æ³•ã€‚"
                    f"é€™å€‹éŒ¯èª¤è¡¨ç¤ºæ¨¡å‹æœŸæœ›çš„è¼¸å…¥ç‰¹å¾µæ•¸é‡èˆ‡å¯¦éš›æä¾›çš„ä¸åŒã€‚"
                )
        
        # æå–ç‰¹å¾µ
        features = self.feature_extractor(x)
        
        # è¼¸å‡ºlogits
        logits = self.output_layer(features)
        
        return {
            'logits': logits,
            'features': features,
            'probabilities': F.softmax(logits, dim=-1)
        }


class AttentionEnhancedClassifier(BaseClassifier):
    """
    æ³¨æ„åŠ›å¢å¼·åˆ†é¡å™¨
    
    ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶å¢å¼·ç‰¹å¾µè¡¨ç¤ºçš„åˆ†é¡å™¨
    """
    
    def __init__(self,
                 input_dim: int,
                 num_classes: int,
                 num_heads: int = 8,
                 hidden_dim: int = 512,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å¢å¼·åˆ†é¡å™¨
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            num_classes: åˆ†é¡æ•¸é‡
            num_heads: æ³¨æ„åŠ›é ­æ•¸
            hidden_dim: éš±è—å±¤ç¶­åº¦
            dropout_rate: Dropoutæ¯”ç‡
        """
        super(AttentionEnhancedClassifier, self).__init__(input_dim, num_classes, dropout_rate)
        
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        
        # è¼¸å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # è‡ªæ³¨æ„åŠ›å±¤
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout_rate,
            batch_first=True
        )
        
        # å‰é¥‹ç¶²è·¯
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # å±¤æ­£è¦åŒ–
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        
        # åˆ†é¡é ­
        self.classifier_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
        # ä½ç½®ç·¨ç¢¼ï¼ˆå¯é¸ï¼‰
        self.positional_encoding = nn.Parameter(torch.randn(1, 1, hidden_dim))
        
        # ä¿å­˜åˆå§‹åŒ–åƒæ•¸
        self._expected_input_dim = input_dim
        
    def reinitialize_for_input_dim(self, actual_input_dim: int):
        """
        æ ¹æ“šå¯¦éš›è¼¸å…¥ç¶­åº¦é‡æ–°åˆå§‹åŒ–ç¶²è·¯ï¼ˆç•¶ç¶­åº¦ä¸åŒ¹é…æ™‚ä½¿ç”¨ï¼‰
        
        Args:
            actual_input_dim: å¯¦éš›è¼¸å…¥ç‰¹å¾µç¶­åº¦
        """
        if self._expected_input_dim != actual_input_dim:
            print(f"âš ï¸  æ³¨æ„åŠ›å¢å¼·åˆ†é¡å™¨ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}")
            print("ğŸ”„ æ­£åœ¨ä½¿ç”¨æ­£ç¢ºç¶­åº¦é‡æ–°åˆå§‹åŒ–æ³¨æ„åŠ›å¢å¼·åˆ†é¡å™¨...")
            
            # ä¿å­˜ç•¶å‰è¨­å‚™
            device = next(self.parameters()).device
            
            # é‡å»ºè¼¸å…¥æŠ•å½±å±¤
            self._expected_input_dim = actual_input_dim
            self.input_projection = nn.Linear(actual_input_dim, self.hidden_dim).to(device)
            
            # é‡å»ºå…¶ä»–å±¤ï¼ˆä¿æŒåŸæœ‰æ¶æ§‹ï¼‰
            self.self_attention = nn.MultiheadAttention(
                embed_dim=self.hidden_dim,
                num_heads=self.num_heads,
                dropout=self.dropout_rate,
                batch_first=True
            ).to(device)
            
            self.feed_forward = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim * 2),
                nn.GELU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim * 2, self.hidden_dim)
            ).to(device)
            
            self.layer_norm1 = nn.LayerNorm(self.hidden_dim).to(device)
            self.layer_norm2 = nn.LayerNorm(self.hidden_dim).to(device)
            
            self.classifier_head = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.num_classes)
            ).to(device)
            
            # é‡æ–°åˆå§‹åŒ–ä½ç½®ç·¨ç¢¼
            self.positional_encoding = nn.Parameter(torch.randn(1, 1, self.hidden_dim).to(device))
            
            return True
        return False
    
    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ [batch_size, input_dim] æˆ– [batch_size, seq_len, input_dim] æˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            åˆ†é¡çµæœå­—å…¸
        """
        # è™•ç†è¼¸å…¥ç‰¹å¾µ
        x = self._process_input_features(x)
        
        # å¦‚æœæ˜¯2Dè¼¸å…¥ï¼Œæ“´å±•ç‚º3D
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
        
        batch_size, seq_len, _ = x.size()
        
        # è¼¸å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch_size, seq_len, hidden_dim]
        
        # æ·»åŠ ä½ç½®ç·¨ç¢¼
        x = x + self.positional_encoding.expand(batch_size, seq_len, -1)
        
        # è‡ªæ³¨æ„åŠ›
        attended, attention_weights = self.self_attention(x, x, x)
        x = self.layer_norm1(x + self.dropout(attended))
        
        # å‰é¥‹ç¶²è·¯
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + self.dropout(ff_output))
        
        # æ± åŒ–ï¼ˆå¦‚æœåºåˆ—é•·åº¦ > 1ï¼‰
        if seq_len > 1:
            # å¹³å‡æ± åŒ–
            pooled_features = torch.mean(x, dim=1)
        else:
            pooled_features = x.squeeze(1)
        
        # åˆ†é¡
        logits = self.classifier_head(pooled_features)
        
        return {
            'logits': logits,
            'features': pooled_features,
            'attention_weights': attention_weights,
            'probabilities': F.softmax(logits, dim=-1)
        }


class HierarchicalClassifier(BaseClassifier):
    """
    éšå±¤å¼åˆ†é¡å™¨
    
    é€²è¡Œç²—ç²’åº¦åˆ°ç´°ç²’åº¦çš„éšå±¤å¼åˆ†é¡
    """
    
    def __init__(self,
                 input_dim: int,
                 coarse_classes: int,
                 fine_classes: int,
                 hidden_dim: int = 512,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–éšå±¤å¼åˆ†é¡å™¨
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            coarse_classes: ç²—ç²’åº¦åˆ†é¡æ•¸
            fine_classes: ç´°ç²’åº¦åˆ†é¡æ•¸
            hidden_dim: éš±è—å±¤ç¶­åº¦
            dropout_rate: Dropoutæ¯”ç‡
        """
        # ä½¿ç”¨fine_classesä½œç‚ºnum_classes
        super(HierarchicalClassifier, self).__init__(input_dim, fine_classes, dropout_rate)
        
        self.coarse_classes = coarse_classes
        self.fine_classes = fine_classes
        self.hidden_dim = hidden_dim
        
        # å…±äº«ç‰¹å¾µæå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # ç²—ç²’åº¦åˆ†é¡å™¨
        self.coarse_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, coarse_classes)
        )
        
        # ç´°ç²’åº¦åˆ†é¡å™¨
        self.fine_classifier = nn.Sequential(
            nn.Linear(hidden_dim + coarse_classes, hidden_dim),  # èåˆç²—åˆ†é¡çµæœ
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, fine_classes)
        )
        
        # ä¿å­˜åˆå§‹åŒ–åƒæ•¸
        self._expected_input_dim = input_dim
        
    def reinitialize_for_input_dim(self, actual_input_dim: int):
        """
        æ ¹æ“šå¯¦éš›è¼¸å…¥ç¶­åº¦é‡æ–°åˆå§‹åŒ–ç¶²è·¯ï¼ˆç•¶ç¶­åº¦ä¸åŒ¹é…æ™‚ä½¿ç”¨ï¼‰
        """
        if self._expected_input_dim != actual_input_dim:
            print(f"âš ï¸  éšå±¤å¼åˆ†é¡å™¨ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}")
            print("ğŸ”„ æ­£åœ¨ä½¿ç”¨æ­£ç¢ºç¶­åº¦é‡æ–°åˆå§‹åŒ–éšå±¤å¼åˆ†é¡å™¨...")
            
            # ä¿å­˜ç•¶å‰è¨­å‚™
            device = next(self.parameters()).device
            
            # é‡å»ºç‰¹å¾µæå–å™¨
            self._expected_input_dim = actual_input_dim
            self.feature_extractor = nn.Sequential(
                nn.Linear(actual_input_dim, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate)
            ).to(device)
            
            # é‡å»ºåˆ†é¡å™¨ï¼ˆä¿æŒåŸæœ‰çµæ§‹ï¼‰
            self.coarse_classifier = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.coarse_classes)
            ).to(device)
            
            self.fine_classifier = nn.Sequential(
                nn.Linear(self.hidden_dim + self.coarse_classes, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim, self.fine_classes)
            ).to(device)
            
            return True
        return False
    
    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ [batch_size, input_dim] æˆ–ç‰¹å¾µå­—å…¸
            
        Returns:
            åˆ†é¡çµæœå­—å…¸
        """
        # è™•ç†è¼¸å…¥ç‰¹å¾µ
        x = self._process_input_features(x)
        
        # ç‰¹å¾µæå–
        features = self.feature_extractor(x)
        
        # ç²—ç²’åº¦åˆ†é¡
        coarse_logits = self.coarse_classifier(features)
        coarse_probs = F.softmax(coarse_logits, dim=-1)
        
        # èåˆç²—åˆ†é¡çµæœé€²è¡Œç´°åˆ†é¡
        fine_input = torch.cat([features, coarse_probs], dim=-1)
        fine_logits = self.fine_classifier(fine_input)
        
        return {
            'logits': fine_logits,  # ä¸»è¦è¼¸å‡ºæ˜¯ç´°åˆ†é¡çµæœ
            'coarse_logits': coarse_logits,
            'fine_logits': fine_logits,
            'features': features,
            'coarse_probabilities': coarse_probs,
            'fine_probabilities': F.softmax(fine_logits, dim=-1),
            'probabilities': F.softmax(fine_logits, dim=-1)  # ä¿æŒèˆ‡åŸºé¡ä¸€è‡´
        }


class CrossDomainClassifier(BaseClassifier):
    """
    è·¨é ˜åŸŸåˆ†é¡å™¨
    
    é‡å°è·¨é ˜åŸŸæƒ…æ„Ÿåˆ†æè¨­è¨ˆçš„åˆ†é¡å™¨
    """
    
    def __init__(self,
                 input_dim: int,
                 num_classes: int,
                 num_domains: int,
                 domain_adaptation: bool = True,
                 hidden_dim: int = 512,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–è·¨é ˜åŸŸåˆ†é¡å™¨
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
            num_classes: åˆ†é¡æ•¸é‡
            num_domains: é ˜åŸŸæ•¸é‡
            domain_adaptation: æ˜¯å¦ä½¿ç”¨é ˜åŸŸé©æ‡‰
            hidden_dim: éš±è—å±¤ç¶­åº¦
            dropout_rate: Dropoutæ¯”ç‡
        """
        super(CrossDomainClassifier, self).__init__(input_dim, num_classes, dropout_rate)
        
        self.num_domains = num_domains
        self.domain_adaptation = domain_adaptation
        self.hidden_dim = hidden_dim
        
        # å…±äº«ç‰¹å¾µæå–å™¨
        self.shared_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # é ˜åŸŸç‰¹å®šç‰¹å¾µæå–å™¨
        if domain_adaptation:
            self.domain_feature_extractors = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout_rate)
                ) for _ in range(num_domains)
            ])
            
            # é ˜åŸŸåˆ¤åˆ¥å™¨ï¼ˆç”¨æ–¼å°æŠ—è¨“ç·´ï¼‰
            self.domain_discriminator = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_dim // 2, num_domains)
            )
            
            feature_dim = hidden_dim
        else:
            feature_dim = hidden_dim
        
        # æƒ…æ„Ÿåˆ†é¡å™¨
        self.sentiment_classifier = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
        # æ¢¯åº¦åè½‰å±¤æ¬Šé‡ï¼ˆç”¨æ–¼å°æŠ—è¨“ç·´ï¼‰
        self.lambda_grl = nn.Parameter(torch.tensor(1.0))
        
        # ä¿å­˜åˆå§‹åŒ–åƒæ•¸
        self._expected_input_dim = input_dim
        
    def reinitialize_for_input_dim(self, actual_input_dim: int):
        """
        æ ¹æ“šå¯¦éš›è¼¸å…¥ç¶­åº¦é‡æ–°åˆå§‹åŒ–ç¶²è·¯ï¼ˆç•¶ç¶­åº¦ä¸åŒ¹é…æ™‚ä½¿ç”¨ï¼‰
        """
        if self._expected_input_dim != actual_input_dim:
            print(f"âš ï¸  è·¨é ˜åŸŸåˆ†é¡å™¨ç¶­åº¦ä¸åŒ¹é…ï¼šé æœŸ {self._expected_input_dim}ï¼Œå¯¦éš›å¾—åˆ° {actual_input_dim}")
            print("ğŸ”„ æ­£åœ¨ä½¿ç”¨æ­£ç¢ºç¶­åº¦é‡æ–°åˆå§‹åŒ–è·¨é ˜åŸŸåˆ†é¡å™¨...")
            
            # ä¿å­˜ç•¶å‰è¨­å‚™
            device = next(self.parameters()).device
            
            # é‡å»ºå…±äº«ç‰¹å¾µæå–å™¨
            self._expected_input_dim = actual_input_dim
            self.shared_feature_extractor = nn.Sequential(
                nn.Linear(actual_input_dim, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate)
            ).to(device)
            
            # é‡å»ºå…¶ä»–å±¤ï¼ˆä¿æŒåŸæœ‰çµæ§‹ï¼‰
            if self.domain_adaptation:
                self.domain_feature_extractors = nn.ModuleList([
                    nn.Sequential(
                        nn.Linear(self.hidden_dim, self.hidden_dim),
                        nn.LayerNorm(self.hidden_dim),
                        nn.ReLU(),
                        nn.Dropout(self.dropout_rate)
                    ).to(device) for _ in range(self.num_domains)
                ])
                
                self.domain_discriminator = nn.Sequential(
                    nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(self.dropout_rate),
                    nn.Linear(self.hidden_dim // 2, self.num_domains)
                ).to(device)
            
            self.sentiment_classifier = nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.num_classes)
            ).to(device)
            
            return True
        return False
    
    def forward(self, 
                x: Union[torch.Tensor, Dict[str, torch.Tensor]], 
                domain_ids: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ [batch_size, input_dim] æˆ–ç‰¹å¾µå­—å…¸
            domain_ids: é ˜åŸŸID [batch_size] ï¼ˆå¯é¸ï¼‰
            
        Returns:
            åˆ†é¡çµæœå­—å…¸
        """
        # è™•ç†è¼¸å…¥ç‰¹å¾µ
        x = self._process_input_features(x)
        
        batch_size = x.size(0)
        
        # å…±äº«ç‰¹å¾µæå–
        shared_features = self.shared_feature_extractor(x)
        
        if self.domain_adaptation and domain_ids is not None:
            # é ˜åŸŸç‰¹å®šç‰¹å¾µæå–
            domain_features = []
            for i in range(batch_size):
                domain_id = domain_ids[i].item()
                if domain_id < len(self.domain_feature_extractors):
                    domain_feature = self.domain_feature_extractors[domain_id](
                        shared_features[i:i+1]
                    )
                else:
                    domain_feature = shared_features[i:i+1]
                domain_features.append(domain_feature)
            
            domain_features = torch.cat(domain_features, dim=0)
            final_features = domain_features
            
            # é ˜åŸŸåˆ¤åˆ¥
            domain_logits = self.domain_discriminator(
                self._gradient_reversal(shared_features)
            )
        else:
            final_features = shared_features
            domain_logits = None
        
        # æƒ…æ„Ÿåˆ†é¡
        sentiment_logits = self.sentiment_classifier(final_features)
        
        result = {
            'logits': sentiment_logits,
            'features': final_features,
            'shared_features': shared_features,
            'probabilities': F.softmax(sentiment_logits, dim=-1)
        }
        
        if domain_logits is not None:
            result.update({
                'domain_logits': domain_logits,
                'domain_probabilities': F.softmax(domain_logits, dim=-1)
            })
        
        return result
    
    def _gradient_reversal(self, x: torch.Tensor) -> torch.Tensor:
        """
        æ¢¯åº¦åè½‰å±¤
        
        Args:
            x: è¼¸å…¥å¼µé‡
            
        Returns:
            æ¢¯åº¦åè½‰å¾Œçš„å¼µé‡
        """
        return GradientReversalFunction.apply(x, self.lambda_grl)


class GradientReversalFunction(torch.autograd.Function):
    """
    æ¢¯åº¦åè½‰å‡½æ•¸
    
    ç”¨æ–¼å°æŠ—è¨“ç·´çš„æ¢¯åº¦åè½‰å±¤
    """
    
    @staticmethod
    def forward(ctx, x, lambda_val):
        ctx.lambda_val = lambda_val
        return x.clone()
    
    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.lambda_val * grad_output, None


class EnsembleClassifier(nn.Module):
    """
    é›†æˆåˆ†é¡å™¨
    
    çµ„åˆå¤šå€‹åˆ†é¡å™¨é€²è¡Œé æ¸¬
    """
    
    def __init__(self,
                 classifiers: List[BaseClassifier],
                 ensemble_method: str = 'average',
                 weights: Optional[List[float]] = None):
        """
        åˆå§‹åŒ–é›†æˆåˆ†é¡å™¨
        
        Args:
            classifiers: åˆ†é¡å™¨åˆ—è¡¨
            ensemble_method: é›†æˆæ–¹æ³• ('average', 'voting', 'learned')
            weights: åˆ†é¡å™¨æ¬Šé‡ï¼ˆå¯é¸ï¼‰
        """
        super(EnsembleClassifier, self).__init__()
        
        self.classifiers = nn.ModuleList(classifiers)
        self.ensemble_method = ensemble_method
        self.num_classifiers = len(classifiers)
        
        # è¨­å®šæ¬Šé‡
        if weights is None:
            self.weights = torch.ones(self.num_classifiers) / self.num_classifiers
        else:
            self.weights = torch.tensor(weights)
            self.weights = self.weights / self.weights.sum()  # æ­£è¦åŒ–
        
        # å¦‚æœæ˜¯å­¸ç¿’å¼é›†æˆï¼Œæ·»åŠ å…ƒå­¸ç¿’å™¨
        if ensemble_method == 'learned':
            # å‡è¨­æ‰€æœ‰åˆ†é¡å™¨æœ‰ç›¸åŒçš„é¡åˆ¥æ•¸
            num_classes = classifiers[0].num_classes
            self.meta_learner = nn.Sequential(
                nn.Linear(num_classes * self.num_classifiers, num_classes * 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(num_classes * 2, num_classes)
            )
    
    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]], **kwargs) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µæˆ–ç‰¹å¾µå­—å…¸
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            é›†æˆåˆ†é¡çµæœ
        """
        # ç²å–æ‰€æœ‰åˆ†é¡å™¨çš„é æ¸¬
        all_outputs = []
        all_probabilities = []
        
        for classifier in self.classifiers:
            output = classifier(x, **kwargs)
            all_outputs.append(output)
            all_probabilities.append(output['probabilities'])
        
        # æ ¹æ“šé›†æˆæ–¹æ³•çµ„åˆçµæœ
        if self.ensemble_method == 'average':
            # åŠ æ¬Šå¹³å‡
            weights = self.weights.to(x.device)
            ensemble_probs = sum(
                w * prob for w, prob in zip(weights, all_probabilities)
            )
            ensemble_logits = torch.log(ensemble_probs + 1e-8)
            
        elif self.ensemble_method == 'voting':
            # å¤šæ•¸æŠ•ç¥¨
            predictions = [torch.argmax(prob, dim=-1) for prob in all_probabilities]
            stacked_preds = torch.stack(predictions, dim=1)  # [batch_size, num_classifiers]
            
            # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æŠ•ç¥¨æ•¸
            batch_size, num_classes = all_probabilities[0].shape
            vote_counts = torch.zeros(batch_size, num_classes, device=x.device)
            
            for i in range(batch_size):
                votes = stacked_preds[i]
                for vote in votes:
                    vote_counts[i, vote] += 1
            
            ensemble_probs = vote_counts / self.num_classifiers
            ensemble_logits = torch.log(ensemble_probs + 1e-8)
            
        elif self.ensemble_method == 'learned':
            # ä½¿ç”¨å…ƒå­¸ç¿’å™¨
            concatenated_probs = torch.cat(all_probabilities, dim=-1)
            ensemble_logits = self.meta_learner(concatenated_probs)
            ensemble_probs = F.softmax(ensemble_logits, dim=-1)
        
        return {
            'logits': ensemble_logits,
            'probabilities': ensemble_probs,
            'individual_outputs': all_outputs
        }
    
    def predict(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        é æ¸¬æ–¹æ³•
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            é æ¸¬æ¨™ç±¤
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(x, **kwargs)
            predictions = torch.argmax(outputs['logits'], dim=-1)
        return predictions
    
    def predict_proba(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        é æ¸¬æ©Ÿç‡æ–¹æ³•
        
        Args:
            x: è¼¸å…¥ç‰¹å¾µ
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            é æ¸¬æ©Ÿç‡
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(x, **kwargs)
            probabilities = outputs['probabilities']
        return probabilities